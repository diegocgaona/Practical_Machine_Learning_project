---
title: "Practical Machine Learning - Project"
author: "Diego Gaona"
date: "11 de novembro de 2015"
output:
  html_document:
    theme: readable
    highlight: espresso
---
```{r load, echo = F, results='hide', message=FALSE, warning=FALSE}
require("knitr")
opts_chunk$set(echo = T, cache = TRUE, fig.width=13, fig.height=6.1)
knitr::opts_chunk$set(tidy=FALSE, fig.path='figures/')
require(caret)
require(randomForest)
require(gbm)
require(survival)
```

##Introduction  
This is a course assignment from Coursera Data Science Specialization, by Johns Hopkins University.
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

**Data**  
The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

**Classification in Classe variable:**  
A.  According to specification  
B   Improper elbow position  
C.  Inadequate range (lower)  
D.  Inadequate range (upper)  
E.  Improper posture  

##Loading and cleaning the Data  
Now, I load the data from the files, transforming all the blank records in NA. I transform the classe variable (the outcome from the prediction) to factors.
After it we see the dimensions of the data loaded.  
```{r loading_data}
rawtraining <- read.csv(file = "pml-training.csv", header = T, stringsAsFactors = F, na.strings=c("NA", "N/A", "", " ", "  "))
rawtraining$classe <- as.factor(rawtraining$classe)
testing <- read.csv(file = "pml-testing.csv", header = T, stringsAsFactors = F, na.strings=c("NA", "N/A", "", " ", "  "))
dim(rawtraining)
dim(testing)
```
After a futher investigation of the data (using "summary(rawtraining)"), we see many NAs, and now I will subset the data excluding all the variables with more than 97% of NA values. After this we delete the variables with near zero variance, that variables would not be important to our predictions.  
```{r cleaning_data}
## Excluding variables with more than 97% of NA
NA_97percent <- sapply(rawtraining, function(x) mean(is.na(x))) < 0.97 
training1 <- subset(rawtraining, select = NA_97percent == 1)
## Excluding variables with near zero variance.
nzvtrain <- nearZeroVar(training1, saveMetrics= TRUE)[4]
training1 <- subset(training1, select = nzvtrain == 0)

```

##Exploring the data with plots
Now, I make a histogram to see the frequency of the classes in the training data set. We see that the most frequent classe is "A" (According to specification), the other classes have a more near frequency.
```{r histogram}
hist1 <- ggplot(training1, aes(x = factor(classe) )) +
      geom_histogram(col = "black", aes(fill =- ..count..)) + 
      ggtitle(expression(bold("Histogram of classe frequency"))) +
      ylab("Frequency") +  xlab("Classe")
hist1
```

##Creating a data partition for cross validation  
Now we create a data partition of the data, for cross validation and performance purposes. How we can't have a separated validation data set, we create a validation data set from our training set, using 30% of the data to validation and see the performance of our models.  
```{r data_partition}
set.seed(855)
inTrain <- createDataPartition(y = training1$classe, p=0.7, list=FALSE)
training <- training1[inTrain,]
mytesting <- training1[-inTrain,]
```

##Creating the models  
Now we create some modeles using the training data, to test with our "mytesting" data. We train and test 3 models. I tried some others, as Random Forest with preprocess PCA, SVN and KNN (very innacurate in this data), but with worst  results on "mytesting", and because of it, I didn't put these models on this final report.  
```{r training_models}
## Random forest model
if( !file.exists('modFitrf_no_pre_pca.rds')) {
    set.seed(855)
    modFitrf_no_pre_pca <- train(classe ~ .,method = "rf", data = training[,3:59])
    saveRDS(modFitrf_no_pre_pca, "modFitrf_no_pre_pca.rds")
    } else {
        modFitrf_no_pre_pca <- readRDS("modFitrf_no_pre_pca.rds")
        }
predictrf_no_pre_pca <- predict(modFitrf_no_pre_pca, newdata = mytesting)

## Stochastic Gradient Boosting Model
if( !file.exists('modFitgbmControl.rds')) {
    set.seed(855)
    modFitgbmControl <- train(classe ~ .,method = "gbm", data = training[,3:59], trControl = trainControl(method="repeatedcv", number=10, repeats=3, savePred=T))
    saveRDS(modFitgbmControl, "modFitgbmControl.rds")
    } else {
        modFitgbmControl <- readRDS("modFitgbmControl.rds")
        }
predictgbmControl <- predict(modFitgbmControl, newdata = mytesting)

## Support Vector Machines with Linear Kernel Model
if( !file.exists('modFitsvm.rds')) {
    set.seed(855)
    modFitsvm <- train(classe ~ .,method = "svmLinear", data = training[,3:59])
    saveRDS(modFitsvm, "modFitsvm.rds")
    } else {
        modFitsvm <- readRDS("modFitsvm.rds")
        }
predsvm <- predict(modFitsvm, newdata = mytesting)

## Confusion matrix to show the accuracy of the models

## ## Random forest model accuracy
confusionMatrix(mytesting$classe, predictrf_no_pre_pca)$overall
## Stochastic Gradient Boosting Model accuracy
confusionMatrix(mytesting$classe, predictgbmControl)$overall
## Support Vector Machines with Linear Kernel Model accuracy
confusionMatrix(mytesting$classe, predsvm)$overall
```

##Selecting the best models  
Now we saw the accurace from our models, based in "mytesting" data, I will select the 2 best models to train with all the training data (not only 70%), and after that, use it on the 20 testing cases. I selected 2 different models to cross validate the final results (the classe from the final testing set). Because the lower accuracy and kappa (0.9082413 and 0.8836277), I rejected the linear SVM model.
My selected model is based on Random Forest, the model with best accuracy (1.0) and I used the Stochastic Gradient Boosting to cross validate the final results.

```{r final_models}
## Random forest Final Model
if( !file.exists('modFitrfFinal.rds')) {
    set.seed(855)
    modFitrfFinal <- train(classe ~ .,method = "rf", data = training[,3:59])
    saveRDS(modFitrfFinal, "modFitrfFinal.rds")
    } else {
        modFitrfFinal <- readRDS("modFitrfFinal.rds")
        }
predictrfFinal <- predict(modFitrfFinal, newdata = testing)

## Stochastic Gradient Boosting Final Model
if( !file.exists('modFitrfFinal.rds')) {
    set.seed(855)
    modFitgbmFinal <- train(classe ~ .,method = "gbm", data = training[,3:59], trControl = trainControl(method="repeatedcv", number=10, repeats=3, savePred=T))
    saveRDS(modFitgbmFinal, "modFitgbmFinal.rds")
    } else {
        modFitgbmFinal <- readRDS("modFitgbmFinal.rds")
        }
predictgbmFinal <- predict(modFitgbmFinal, newdata = testing)
```
## Plots for the 2 best models
Now, we see plots related to the models, related to their accuracy.
```{r final_models_plot}
ggplot(modFitrfFinal)
ggplot(modFitgbmFinal)
```

## Final predictions
Now we see the prediction results of both final models, to compare the results of the different methods, and crosse validate. How we see the results are identical to both models, and these are my final predictions to submit to Coursera.
```{r final_predictions}
predictrfFinal
predictgbmFinal
all.equal(predictrfFinal,predictgbmFinal)
```

##Creating files to submit to Coursera
Now, I create the files to submit to Coursera, to verify if my predictions are correct.
```{r create_files}
pml_write_files = function(x){
      n = length(x)
      for(i in 1:n){
            filename = paste0("problem_id_",i,".txt")
            write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
      }
}
pml_write_files(predictrfFinal)
```

##Conclusion and out of sample error
After creating and testing the models, I selected the Random Forest model, because it have accuracy and Kappa of 1.00.
According to Professor Leek, the out of sample error is: "error rate you get on new data set.". In my model, the **out of sample error would be 0 (1-1)**, what is a little strange, and I think is related to the data, and how the training and mytesting is related.
But the model is accurate, because the classes on the "testing" data are all correct.
In the case of the GBM model, the out of sample error is 0.0028887 (1-0.9971113), which is very accurated too.